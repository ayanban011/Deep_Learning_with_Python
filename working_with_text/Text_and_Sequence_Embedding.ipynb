{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Word level one hot encoding**"
      ],
      "metadata": {
        "id": "atxxpXRykh_i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PwFSS2QQpQFo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aec028b4-7e45-44f7-984e-8990647121e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "samples = ['The cat sat on the mat.', 'The dog ate my homework'] #Intial data: one entry per sample \n",
        "token_index = {} #Builds an index of all tokens in the data\n",
        "for sample in samples:\n",
        "  for word in sample.split(): #Tokenize the sample via the split method. This strip can also be done with punctuation and special characters.\n",
        "    if word not in token_index:\n",
        "      token_index[word] = len(token_index)+1 #Assigns a unique index to each unique words except 0.\n",
        "max_length = 10 #For vectorizing the sample we are only considering the first max_length word in each sample.\n",
        "results = np.zeros(shape=(len(samples), max_length, max(token_index.values())+1)) #This is where we store the results\n",
        "for i,sample in enumerate(samples):\n",
        "  for j,word in list(enumerate(sample.split()))[:max_length]:\n",
        "    index = token_index.get(word)\n",
        "    results[i,j,index] = 1\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Character level one hot encoding**"
      ],
      "metadata": {
        "id": "InSuJr9boJWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "samples = ['The cat sat on the mat', 'The dog ate my homework']\n",
        "characters = string.printable #All printable ASCII characters\n",
        "token_index = dict(zip(range(1, len(characters)+1), characters))\n",
        "\n",
        "max_length = 50\n",
        "results = np.zeros((len(samples), max_length, max(token_index.keys())+1))\n",
        "for i, sample in enumerate(samples):\n",
        "  for j,character in enumerate(sample):\n",
        "    index  = token_index.get(character)\n",
        "    results[i,j,index] = 1\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7C3c6tlpTET",
        "outputId": "85f16a56-3884-41e0-8028-b747d40f970c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> **Note:** Keras has built-in utilities for doing one-hot encoding of the text at the word level or character level, strating from raw text data. You should use these utilities, because they take care of a number of important features such as stripping special characters from strings and onlt taken into account the N most common words in our dataset (It is a common  restriction to avoid dealing with very large input vector spaces).\n",
        "\n"
      ],
      "metadata": {
        "id": "aPZCUgQLqRp2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Using Keras for word level one-hot encoding**"
      ],
      "metadata": {
        "id": "WTFomAwlrQj0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
        "tokenizer = Tokenizer(num_words=1000) #Creates a tokenizer, configured to only take into account the 1,000 most common words\n",
        "tokenizer.fit_on_texts(samples) #Builds the word index\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(samples) #Turn strings into lists of integer indices\n",
        "one_hot_results = tokenizer.texts_to_matrix(samples,mode='binary') #directly get the one-hot binary representations\n",
        "\n",
        "word_index = tokenizer.word_index #recover the word index that was computed\n",
        "print('Found %s unique tokens.' % len(word_index), word_index)"
      ],
      "metadata": {
        "id": "pA9NbhzapgMr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "017f3996-ca1b-4f3f-c267-96cf2163c109"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 9 unique tokens. {'the': 1, 'cat': 2, 'sat': 3, 'on': 4, 'mat': 5, 'dog': 6, 'ate': 7, 'my': 8, 'homework': 9}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **One-hot encoding with hashing trick**"
      ],
      "metadata": {
        "id": "cw9DLVTYuTuC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
        "dimensionality = 1000\n",
        "max_length = 10\n",
        "results = np.zeros((len(samples), max_length, dimensionality))\n",
        "for i,sample in enumerate(samples):\n",
        "  for j,word in list(enumerate(sample.split()))[:max_length]:\n",
        "    index = abs(hash(word)) % dimensionality #Hashes the word into a random integer index between 0 and 1000\n",
        "    results[i,j,index] =1\n",
        "    print(results[i,:,:],index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6FmfF-qtXsb",
        "outputId": "461e68e6-5f01-4f80-d643-38462e1b130a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]] 865\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]] 755\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]] 288\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]] 943\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]] 579\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]] 40\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]] 865\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]] 912\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]] 451\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]] 575\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]] 293\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Instantiating an Embedding Layer**\n",
        "It works as a dictionary look up that maps integer indices (stand for specific words) to dense vectors. It takes integers as input, it looks up these integers in an internal dictionary, and it returns the associated vectors.\n",
        "\n",
        "The embedding layer takes a 2D tensor of integers, of shape = `(samples, sequence length)` where each entry isa sequence of integers. It can embed the sequence of variable length but all sequences in a batch must have same length. In order to do this padding and truncation has been performed for short and long sequences respectively.\n",
        "\n",
        "It returns a 3D floating-point tensor  of shape` (samples, sequence_length, embedding_dimensionality)`."
      ],
      "metadata": {
        "id": "OQmCdpIj2chm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **loading the IMDB data for use with an Embedding Layer**"
      ],
      "metadata": {
        "id": "lYM4ty6e4bNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import imdb\n",
        "from keras import preprocessing\n",
        "\n",
        "max_features = 10000 #Number of words to consider as features\n",
        "maxlen = 20 #Cuts off the text after this number of words\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen) #Turns the lists of integers into a 2D integer tensor of shape (samples, maxlen)\n",
        "x_train = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)"
      ],
      "metadata": {
        "id": "nl16a_9rvvqM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f643021a-d5c0-4d4e-afac-81ff9046df5b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "17473536/17464789 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten, Dense\n",
        "from keras.layers import Embedding\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(10000, 8, input_length=maxlen)) #The activation shape (samples, maxlen, 8)\n",
        "model.add(Flatten()) #shape(samples, maxlen*8)\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fqsscnvt5-Do",
        "outputId": "45c6e2dc-85e6-493a-c58e-89556a8a8077"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 20, 8)             80000     \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 160)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 161       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 80,161\n",
            "Trainable params: 80,161\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 6s 3ms/step - loss: 0.6932 - acc: 0.5062 - val_loss: 0.6933 - val_acc: 0.5020\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.6813 - acc: 0.6129 - val_loss: 0.6950 - val_acc: 0.5016\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.6585 - acc: 0.6670 - val_loss: 0.7005 - val_acc: 0.5010\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.6261 - acc: 0.7036 - val_loss: 0.7112 - val_acc: 0.5026\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.5886 - acc: 0.7354 - val_loss: 0.7271 - val_acc: 0.5024\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.5495 - acc: 0.7624 - val_loss: 0.7470 - val_acc: 0.5038\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.5115 - acc: 0.7825 - val_loss: 0.7705 - val_acc: 0.5012\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.4756 - acc: 0.7998 - val_loss: 0.7958 - val_acc: 0.5044\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.4426 - acc: 0.8171 - val_loss: 0.8238 - val_acc: 0.5032\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.4127 - acc: 0.8324 - val_loss: 0.8532 - val_acc: 0.5062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> **Note:** Though the validation accuracy is pretty good but this model treat each word in the input sequence separately, without considering inter-word relationship and sentence structure (For example, \"This movie is a dumb\" and \"This movie is about the dumb\" are treated as negative reviews which is not correct).\n",
        "\n"
      ],
      "metadata": {
        "id": "VKFFctGC7xnh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Playing with raw IMDB data**\n",
        "First, head to http://mng.bz/0tIo and download the data. But this time we will use the pretrained word embeddings model called `Glove`"
      ],
      "metadata": {
        "id": "Sa9odQtbIijV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Processing the labels of the raw IMDB data\n",
        "import os\n",
        "imdb_dir = '/content/drive/MyDrive/IMDB_raw/aclImdb'\n",
        "train_dir = os.path.join(imdb_dir, 'train')\n",
        "labels = []\n",
        "texts = []\n",
        "\n",
        "for label_type in ['neg', 'pos']:\n",
        "  dir_name = os.path.join(train_dir, label_type)\n",
        "  for fname in os.listdir(dir_name):\n",
        "    if(fname[-4:]=='.txt'):\n",
        "      f=open(os.path.join(dir_name, fname))\n",
        "      texts.append(f.read())\n",
        "      f.close()\n",
        "      if(label_type == 'neg'):\n",
        "        labels.append(0)\n",
        "      else:\n",
        "        labels.append(1)"
      ],
      "metadata": {
        "id": "0-ZcYYJ4Uzfi"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tokenizing the Data**\n",
        "Let's vectorize the text and prepare a `train-val` split, because the pretrained word embeddings are meant to be particularly useful on problems where little training data is available (otherwise, task-specific embeddings are likely to outperform them), we'll add the following twist: restricting the training data to the first 200 samples. So you'll learn to classify movie reviews after looking at just 200 examples."
      ],
      "metadata": {
        "id": "ltnUITSYKl9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "maxlen = 100 #Cuts off reviews after 100 words\n",
        "training_samples = 200 #Trains on 200 samples\n",
        "val_samples = 10000 #Validates on 10,000 samples\n",
        "max_words = 10000 #Considers only top 10,000 words in the dataset\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "data = pad_sequences(sequences,maxlen=maxlen)\n",
        "\n",
        "labels = np.asarray(labels)\n",
        "print('Shape of data tensor:', data.shape)\n",
        "print('Shape of label tensor:', labels.shape)\n",
        "\n",
        "indices = np.arange(data.shape[0]) #Split the data into train-val set\n",
        "np.random.shuffle(indices)          #shuffles the data as the samples are ordered\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "\n",
        "x_train = data[:training_samples]\n",
        "y_train= labels[:training_samples]\n",
        "x_val = data[training_samples: training_samples+val_samples]\n",
        "y_val = labels[training_samples: training_samples+val_samples]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUbjiZ3IKINE",
        "outputId": "b79ca8ef-8dff-4658-be6a-94c549540adf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 27596 unique tokens.\n",
            "Shape of data tensor: (2435, 100)\n",
            "Shape of label tensor: (2435,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Parsing the GloVe word-embedding file\n",
        "glove_dir = '/content/drive/MyDrive/glove'\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
        "for line in f:\n",
        "  values = line.split()\n",
        "  word = values[0]\n",
        "  coefs = np.asarray(values[1:], dtype='float32')\n",
        "  embeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('Found %s word vectors.' %len(embeddings_index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syVpTmXdSEj_",
        "outputId": "82734f22-6818-4875-a8c3-a36f28c7c60a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Preparing the GloVe word-embedding matrix\n",
        "embedding_dim = 100\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "  if(i<max_words):\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if(embedding_vector is not None):\n",
        "      embedding_matrix[i] = embedding_vector\n",
        "#words not found in the embedding index will be all zeros"
      ],
      "metadata": {
        "id": "tUrn2qlWU9Gx"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model definition\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Flatten, Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ufTwhOu8vj5",
        "outputId": "5eb9cc3d-1940-40d4-c616-3a4b56f80848"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 100, 100)          1000000   \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 10000)             0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 32)                320032    \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,320,065\n",
            "Trainable params: 1,320,065\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading the pretrained word embeddings into the embedding layer\n",
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = False"
      ],
      "metadata": {
        "id": "DGdM35PP9i_K"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training and Evaluation\n",
        "model.compile(\n",
        "    optimizer = 'rmsprop',\n",
        "    loss = 'binary_crossentropy',\n",
        "    metrics=['acc']\n",
        ")\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs = 20,\n",
        "                    batch_size = 64,\n",
        "                    validation_data = (x_val,y_val))\n",
        "model.save_weights('pre_trained_glove_model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8BTj1s9-jmc",
        "outputId": "97dfda38-3e09-42fd-b063-ba875bf57b46"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "4/4 [==============================] - 1s 69ms/step - loss: 0.0616 - acc: 1.0000 - val_loss: 4.2011e-05 - val_acc: 1.0000\n",
            "Epoch 2/20\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 1.7865e-07 - acc: 1.0000 - val_loss: 4.2009e-05 - val_acc: 1.0000\n",
            "Epoch 3/20\n",
            "4/4 [==============================] - 0s 57ms/step - loss: 1.7818e-07 - acc: 1.0000 - val_loss: 4.2006e-05 - val_acc: 1.0000\n",
            "Epoch 4/20\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 1.7758e-07 - acc: 1.0000 - val_loss: 4.2002e-05 - val_acc: 1.0000\n",
            "Epoch 5/20\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 1.7693e-07 - acc: 1.0000 - val_loss: 4.1998e-05 - val_acc: 1.0000\n",
            "Epoch 6/20\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 1.7613e-07 - acc: 1.0000 - val_loss: 4.1992e-05 - val_acc: 1.0000\n",
            "Epoch 7/20\n",
            "4/4 [==============================] - 0s 56ms/step - loss: 1.7518e-07 - acc: 1.0000 - val_loss: 4.1985e-05 - val_acc: 1.0000\n",
            "Epoch 8/20\n",
            "4/4 [==============================] - 0s 57ms/step - loss: 1.7402e-07 - acc: 1.0000 - val_loss: 4.1977e-05 - val_acc: 1.0000\n",
            "Epoch 9/20\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 1.7265e-07 - acc: 1.0000 - val_loss: 4.1968e-05 - val_acc: 1.0000\n",
            "Epoch 10/20\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 1.7111e-07 - acc: 1.0000 - val_loss: 4.1957e-05 - val_acc: 1.0000\n",
            "Epoch 11/20\n",
            "4/4 [==============================] - 0s 57ms/step - loss: 1.6939e-07 - acc: 1.0000 - val_loss: 4.1943e-05 - val_acc: 1.0000\n",
            "Epoch 12/20\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 1.6738e-07 - acc: 1.0000 - val_loss: 4.1925e-05 - val_acc: 1.0000\n",
            "Epoch 13/20\n",
            "4/4 [==============================] - 0s 58ms/step - loss: 1.6482e-07 - acc: 1.0000 - val_loss: 4.1905e-05 - val_acc: 1.0000\n",
            "Epoch 14/20\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 1.6201e-07 - acc: 1.0000 - val_loss: 4.1882e-05 - val_acc: 1.0000\n",
            "Epoch 15/20\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 1.5891e-07 - acc: 1.0000 - val_loss: 4.1853e-05 - val_acc: 1.0000\n",
            "Epoch 16/20\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 1.5521e-07 - acc: 1.0000 - val_loss: 4.1820e-05 - val_acc: 1.0000\n",
            "Epoch 17/20\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 1.5120e-07 - acc: 1.0000 - val_loss: 4.1782e-05 - val_acc: 1.0000\n",
            "Epoch 18/20\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 1.4669e-07 - acc: 1.0000 - val_loss: 4.1733e-05 - val_acc: 1.0000\n",
            "Epoch 19/20\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 1.4120e-07 - acc: 1.0000 - val_loss: 4.1677e-05 - val_acc: 1.0000\n",
            "Epoch 20/20\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 1.3542e-07 - acc: 1.0000 - val_loss: 4.1614e-05 - val_acc: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting the results\n",
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc)+1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label = 'Validation acc')\n",
        "plt.title('Training and Validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label = 'Validation loss')\n",
        "plt.title('Training and Validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "2mJBc0lj_VHu",
        "outputId": "f5643d81-e6a3-41fc-d0b8-fdc5eafc8342"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Training the same model without pretraining word embeddings\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Flatten, Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(32,activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics = ['acc'])\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(x_val,y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWO4MyCPAiE7",
        "outputId": "482fdfc4-c9d8-417a-81ce-e332532401fa"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 100, 100)          1000000   \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 10000)             0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 32)                320032    \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,320,065\n",
            "Trainable params: 1,320,065\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "7/7 [==============================] - 1s 47ms/step - loss: 0.4833 - acc: 0.9150 - val_loss: 0.1231 - val_acc: 1.0000\n",
            "Epoch 2/10\n",
            "7/7 [==============================] - 0s 30ms/step - loss: 0.0520 - acc: 1.0000 - val_loss: 0.0284 - val_acc: 1.0000\n",
            "Epoch 3/10\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0128 - acc: 1.0000 - val_loss: 0.0120 - val_acc: 1.0000\n",
            "Epoch 4/10\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0054 - acc: 1.0000 - val_loss: 0.0062 - val_acc: 1.0000\n",
            "Epoch 5/10\n",
            "7/7 [==============================] - 0s 30ms/step - loss: 0.0027 - acc: 1.0000 - val_loss: 0.0035 - val_acc: 1.0000\n",
            "Epoch 6/10\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.0021 - val_acc: 1.0000\n",
            "Epoch 7/10\n",
            "7/7 [==============================] - 0s 31ms/step - loss: 8.7006e-04 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000\n",
            "Epoch 8/10\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 5.2250e-04 - acc: 1.0000 - val_loss: 8.4636e-04 - val_acc: 1.0000\n",
            "Epoch 9/10\n",
            "7/7 [==============================] - 0s 31ms/step - loss: 3.2086e-04 - acc: 1.0000 - val_loss: 5.4880e-04 - val_acc: 1.0000\n",
            "Epoch 10/10\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 2.0003e-04 - acc: 1.0000 - val_loss: 3.6488e-04 - val_acc: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenizing the data of the test set\n",
        "test_dir = os.path.join(imdb_dir, 'test')\n",
        "labels = []\n",
        "texts = []\n",
        "for label_type in ['neg', 'pos']:\n",
        "  dir_name = os.path.join(test_dir, label_type)\n",
        "  for fname in sorted(os.listdir(dir_name)):\n",
        "    if(fname[-4:] == '.txt'):\n",
        "      f = open(os.path.join(dir_name, fname))\n",
        "      texts.append(f.read())\n",
        "      f.close()\n",
        "      if(label_type == 'neg'):\n",
        "        labels.append(0)\n",
        "      else:\n",
        "        labels.append(1)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "x_test = pad_sequences(sequences, maxlen=maxlen)\n",
        "y_test = np.asarray(labels)"
      ],
      "metadata": {
        "id": "6uK8i5k0CeVD"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluating the model on the test set\n",
        "model.load_weights('pre_trained_glove_model.h5')\n",
        "model.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJrY8EBAD-z3",
        "outputId": "d37b00bb-a35d-4a31-9adf-ebd9657265e7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 2s 3ms/step - loss: 16.2883 - acc: 0.5000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[16.288259506225586, 0.5]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lTLS1jSXHEBg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}