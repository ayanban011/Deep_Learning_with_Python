{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Representing_text_as_tensors.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNdOhOsmnn5YlAAvynIpCPS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayanban011/Getting-Started_with_Pytorch/blob/main/Representing_text_as_tensors.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we want to solve Natural Language Processing (NLP) tasks with neural networks, we need some way to represent text as tensors. Computers already represent textual characters as numbers that map to fonts on your screen using encodings such as ASCII or UTF-8."
      ],
      "metadata": {
        "id": "wkT7IKtCoVlB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-7EDRKwm7Dh",
        "outputId": "02234f2c-8a0f-4aac-ae88-e5edd6db5a23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim==3.8.3 in /usr/local/lib/python3.7/dist-packages (from -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 1)) (3.8.3)\n",
            "Requirement already satisfied: huggingface==0.0.1 in /usr/local/lib/python3.7/dist-packages (from -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 2)) (0.0.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 3)) (3.2.2)\n",
            "Requirement already satisfied: nltk==3.5 in /usr/local/lib/python3.7/dist-packages (from -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 4)) (3.5)\n",
            "Requirement already satisfied: numpy==1.18.5 in /usr/local/lib/python3.7/dist-packages (from -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 5)) (1.18.5)\n",
            "Requirement already satisfied: opencv-python==4.5.1.48 in /usr/local/lib/python3.7/dist-packages (from -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 6)) (4.5.1.48)\n",
            "Requirement already satisfied: Pillow==7.1.2 in /usr/local/lib/python3.7/dist-packages (from -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 7)) (7.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 8)) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 9)) (1.7.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 10)) (1.8.1)\n",
            "Requirement already satisfied: torchaudio==0.8.1 in /usr/local/lib/python3.7/dist-packages (from -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 11)) (0.8.1)\n",
            "Requirement already satisfied: torchinfo==0.0.8 in /usr/local/lib/python3.7/dist-packages (from -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 12)) (0.0.8)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.7/dist-packages (from -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 13)) (0.9.1)\n",
            "Requirement already satisfied: torchvision==0.9.1 in /usr/local/lib/python3.7/dist-packages (from -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 14)) (0.9.1)\n",
            "Requirement already satisfied: transformers==4.3.3 in /usr/local/lib/python3.7/dist-packages (from -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 15)) (4.3.3)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.3->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.3->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 1)) (5.2.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from nltk==3.5->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 4)) (2022.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk==3.5->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 4)) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk==3.5->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 4)) (4.64.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk==3.5->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 4)) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 10)) (4.1.1)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.3->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 15)) (0.10.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.3->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 15)) (4.12.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.3->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 15)) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.3->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 15)) (0.0.53)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.3->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 15)) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.3->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 15)) (3.7.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 3)) (1.4.3)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 3)) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 3)) (0.11.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 8)) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.3.3->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 15)) (3.8.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.3.3->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 15)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.3.3->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 15)) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.3.3->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 15)) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.3.3->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 15)) (2.10)\n"
          ]
        }
      ],
      "source": [
        "#Installing necessary modules to run this notebook\n",
        "!pip install -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Task: Text Classification**\n",
        "In this notebook, we will start with a simple text classification task based on **AG_NEWS** sample dataset, which is to classify news headlines into one of 4 categories: *World, Sports, Business and Sci/Tech*. This dataset is built from PyTorch's ***torchtext*** module, so we can easily access it."
      ],
      "metadata": {
        "id": "o8byMRhVqb6R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchtext\n",
        "import os\n",
        "import collections\n",
        "os.makedirs('./data',exist_ok=True)\n",
        "train_dataset, test_dataset = torchtext.datasets.AG_NEWS(root='./data')\n",
        "classes = ['World', 'Sports', 'Business', 'Sci/Tech']"
      ],
      "metadata": {
        "id": "FyMbhPK8opLC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, `train_dataset` and `test_dataset` contain iterators that return pairs of label (number of class) and text respectively, for example:"
      ],
      "metadata": {
        "id": "mC743kxqrHM5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "next(train_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsnOCv-pq6IV",
        "outputId": "08c38749-9b43-4ac0-8e44-6244093890ea"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3,\n",
              " \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, let's print out somw news headlines from our dataset:"
      ],
      "metadata": {
        "id": "797IitKmrVz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i,x in zip(range(5),train_dataset):\n",
        "    print(f\"**{classes[x[0]]}** -> {x[1]}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgdXpJHCrOU7",
        "outputId": "2c4fff6d-c5be-48be-9462-ed40a40c81ef"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Sci/Tech** -> Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\which has a reputation for making well-timed and occasionally\\controversial plays in the defense industry, has quietly placed\\its bets on another part of the market.\n",
            "\n",
            "**Sci/Tech** -> Oil and Economy Cloud Stocks' Outlook (Reuters) Reuters - Soaring crude prices plus worries\\about the economy and the outlook for earnings are expected to\\hang over the stock market next week during the depth of the\\summer doldrums.\n",
            "\n",
            "**Sci/Tech** -> Iraq Halts Oil Exports from Main Southern Pipeline (Reuters) Reuters - Authorities have halted oil export\\flows from the main pipeline in southern Iraq after\\intelligence showed a rebel militia could strike\\infrastructure, an oil official said on Saturday.\n",
            "\n",
            "**Sci/Tech** -> Oil prices soar to all-time record, posing new menace to US economy (AFP) AFP - Tearaway world oil prices, toppling records and straining wallets, present a new economic menace barely three months before the US presidential elections.\n",
            "\n",
            "**Sci/Tech** -> Stocks End Up, But Near Year Lows (Reuters) Reuters - Stocks ended slightly higher on Friday\\but stayed near lows for the year as oil prices surged past  #36;46\\a barrel, offsetting a positive outlook from computer maker\\Dell Inc. (DELL.O)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because datasets are iterators, if we want to use the data multiple times we need to convert it to a list: (I personally face the problem here to deal with it dwithout converting a list, after one run it always throws the error: List index is out of range)"
      ],
      "metadata": {
        "id": "HAvvsZyXsmwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset, test_dataset = torchtext.datasets.AG_NEWS(root='./data')\n",
        "train_dataset = list(train_dataset)\n",
        "test_dataset = list(test_dataset)"
      ],
      "metadata": {
        "id": "tFXfccgTrpJd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tokenization and Vectorization**\n",
        "\n",
        "Now we need to convert text into **numbers** that can be represented as tensors to feed them into a neural network. The first step is to convert text to tokens - **tokenization**. If we use word-level representation, each word would be represented by its own token. We will use build-in tokenizer from `torchtext` module:"
      ],
      "metadata": {
        "id": "hCiO08GJtvdf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')"
      ],
      "metadata": {
        "id": "MtTNV-BvtdLr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll use PyTorch's tokenizer to split words and spaces in the first 2 news articles. In our case, we use basic_english for the tokenizer to understand the language structure. This will return a string list of the text and characters."
      ],
      "metadata": {
        "id": "LMlpUSiyuG3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "first_sentence = train_dataset[0][1]\n",
        "second_sentence = train_dataset[1][1]\n",
        "\n",
        "f_tokens = tokenizer(first_sentence)\n",
        "s_tokens = tokenizer(second_sentence)\n",
        "\n",
        "print(f'\\nfirst token list:\\n{f_tokens}')\n",
        "print(f'\\nsecond token list:\\n{s_tokens}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exN-7p3at-yq",
        "outputId": "ab7b828c-079c-4247-b65b-83163f37c026"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "first token list:\n",
            "['wall', 'st', '.', 'bears', 'claw', 'back', 'into', 'the', 'black', '(', 'reuters', ')', 'reuters', '-', 'short-sellers', ',', 'wall', 'street', \"'\", 's', 'dwindling\\\\band', 'of', 'ultra-cynics', ',', 'are', 'seeing', 'green', 'again', '.']\n",
            "\n",
            "second token list:\n",
            "['carlyle', 'looks', 'toward', 'commercial', 'aerospace', '(', 'reuters', ')', 'reuters', '-', 'private', 'investment', 'firm', 'carlyle', 'group', ',', '\\\\which', 'has', 'a', 'reputation', 'for', 'making', 'well-timed', 'and', 'occasionally\\\\controversial', 'plays', 'in', 'the', 'defense', 'industry', ',', 'has', 'quietly', 'placed\\\\its', 'bets', 'on', 'another', 'part', 'of', 'the', 'market', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, to convert text to numbers, we will need to build a vocabulary of all tokens. We first build the dictionary using the Counter object, and then create a Vocab object that would help us deal with vectorization:"
      ],
      "metadata": {
        "id": "a6Iun0cYu2V8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "counter = collections.Counter()\n",
        "for (label, line) in train_dataset:\n",
        "    counter.update(tokenizer(line))\n",
        "vocab = torchtext.vocab.Vocab(counter, min_freq=1)"
      ],
      "metadata": {
        "id": "vB1yoiv5uN4A"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see how each word maps to the vocabulary, we'll loop through each word in the list to lookup it's index number in vocab. Each word or character is displayed with it's corresponding index. For example, word 'the' appears several times in both sentence and it's unique index in the vocab is the number 3."
      ],
      "metadata": {
        "id": "JMYOOvtfvEbG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_lookup = [list((vocab[w], w)) for w in f_tokens]\n",
        "print(f'\\nIndex lockup in 1st sentence:\\n{word_lookup}')\n",
        "\n",
        "word_lookup = [list((vocab[w], w)) for w in s_tokens]\n",
        "print(f'\\nIndex lockup in 2nd sentence:\\n{word_lookup}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9BNXe_8u8KR",
        "outputId": "a443605a-6898-4daf-d030-215019c0b4ce"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Index lockup in 1st sentence:\n",
            "[[432, 'wall'], [426, 'st'], [2, '.'], [1606, 'bears'], [14839, 'claw'], [114, 'back'], [67, 'into'], [3, 'the'], [849, 'black'], [14, '('], [28, 'reuters'], [15, ')'], [28, 'reuters'], [16, '-'], [50726, 'short-sellers'], [4, ','], [432, 'wall'], [375, 'street'], [17, \"'\"], [10, 's'], [67508, 'dwindling\\\\band'], [7, 'of'], [52259, 'ultra-cynics'], [4, ','], [43, 'are'], [4010, 'seeing'], [784, 'green'], [326, 'again'], [2, '.']]\n",
            "\n",
            "Index lockup in 2nd sentence:\n",
            "[[15875, 'carlyle'], [1073, 'looks'], [855, 'toward'], [1311, 'commercial'], [4251, 'aerospace'], [14, '('], [28, 'reuters'], [15, ')'], [28, 'reuters'], [16, '-'], [930, 'private'], [798, 'investment'], [321, 'firm'], [15875, 'carlyle'], [99, 'group'], [4, ','], [27658, '\\\\which'], [29, 'has'], [6, 'a'], [4460, 'reputation'], [12, 'for'], [565, 'making'], [52791, 'well-timed'], [9, 'and'], [80618, 'occasionally\\\\controversial'], [2126, 'plays'], [8, 'in'], [3, 'the'], [526, 'defense'], [242, 'industry'], [4, ','], [29, 'has'], [3891, 'quietly'], [82815, 'placed\\\\its'], [6575, 'bets'], [11, 'on'], [207, 'another'], [360, 'part'], [7, 'of'], [3, 'the'], [127, 'market'], [2, '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using vocabulary, we can easily encode our tokenized string into a set of numbers. Let's use the first news article as an example:"
      ],
      "metadata": {
        "id": "swxTuN3Avdc1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(vocab)\n",
        "print(f\"Vocab size of {vocab_size}\")\n",
        "\n",
        "def encode(x):\n",
        "    return [vocab.stoi[s] for s in tokenizer(x)]\n",
        "\n",
        "vec = encode(first_sentence)\n",
        "print(vec)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHPYZTiPvJm1",
        "outputId": "934b757d-bdf9-437c-e398-1cba33859b65"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size of 95812\n",
            "[432, 426, 2, 1606, 14839, 114, 67, 3, 849, 14, 28, 15, 28, 16, 50726, 4, 432, 375, 17, 10, 67508, 7, 52259, 4, 43, 4010, 784, 326, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code, the torchtext vocab.stoi dictionary allows us to convert from a string representation into numbers (the name stoi stands for \"from string to integers). To convert the text back from a numeric representation into text, we can use the vocab.itos dictionary to perform reverse lookup:"
      ],
      "metadata": {
        "id": "oR1-5gBOvooh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decode(x):\n",
        "    return [vocab.itos[i] for i in x]\n",
        "\n",
        "decode(vec)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZN6lCp_Eviu1",
        "outputId": "6dd2d398-f917-4188-ba42-4625fb1714f6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['wall',\n",
              " 'st',\n",
              " '.',\n",
              " 'bears',\n",
              " 'claw',\n",
              " 'back',\n",
              " 'into',\n",
              " 'the',\n",
              " 'black',\n",
              " '(',\n",
              " 'reuters',\n",
              " ')',\n",
              " 'reuters',\n",
              " '-',\n",
              " 'short-sellers',\n",
              " ',',\n",
              " 'wall',\n",
              " 'street',\n",
              " \"'\",\n",
              " 's',\n",
              " 'dwindling\\\\band',\n",
              " 'of',\n",
              " 'ultra-cynics',\n",
              " ',',\n",
              " 'are',\n",
              " 'seeing',\n",
              " 'green',\n",
              " 'again',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **BiGrams, TriGrams and N-Grams**\n",
        "\n",
        "One limitation of word tokenization is that some words are part of multi word expressions, for example, the word _'hot dog'_ has a completely different meaning than the words 'hot' and 'dog' in other contexts. If we represent words 'hot` and 'dog' always by the same vectors, it can confuse the model.\n",
        "\n",
        "To address this, **N-gram representations** are sometimes used in document classification, where the frequency of each word, bi-word or tri-word is a useful feature for training classifiers. \n",
        "- In **bigram** representation, for example, we will add all _word pairs_ to the vocabulary, in addition to original words. \n",
        "\n",
        "To get n-gram representation, we can use `ngrams_iterator` function that will convert the sequence of tokens to the sequence of n-grams. In the code below, we will build bigram vocabulary from our news dataset:"
      ],
      "metadata": {
        "id": "7MCdinQbwP-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data.utils import ngrams_iterator\n",
        "\n",
        "bi_counter = collections.Counter()\n",
        "for (label, line) in train_dataset:\n",
        "    bi_counter.update(ngrams_iterator(tokenizer(line),ngrams=2))\n",
        "bi_vocab = torchtext.vocab.Vocab(bi_counter, min_freq=2)\n",
        "\n",
        "print(f\"Bigram vocab size = {len(bi_vocab)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yg2b0WtfvtJ3",
        "outputId": "98d3af5c-31a8-49c7-f2f4-298b781d36e2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bigram vocab size = 481971\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(x):\n",
        "    return [bi_vocab.stoi[s] for s in tokenizer(x)]\n",
        "\n",
        "bi = encode(first_sentence)"
      ],
      "metadata": {
        "id": "MnO9XIjcwgt8"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main drawback of N-gram approach is that vocabulary size starts to grow extremely fast. Here we specify `min_freq` flag to `Vocab` constructor in order to avoid those tokens that appear in the text only once. We can also increase `min_freq` even further, because infrequent words/phrases usually have little effect on the accuracy of classification.\n",
        "\n",
        "In practice, n-gram vocabulary size is still too high to represent words as one-hot vectors, and thus we need to combine this representation with some dimensionality reduction techniques, such as *embeddings*, which we will discuss in a later unit."
      ],
      "metadata": {
        "id": "WgKb33m7wua3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Tri-gram\n",
        "from torchtext.data.utils import ngrams_iterator\n",
        "\n",
        "tri_counter = collections.Counter()\n",
        "for (label, line) in train_dataset:\n",
        "    tri_counter.update(ngrams_iterator(tokenizer(line),ngrams=3))\n",
        "tri_vocab = torchtext.vocab.Vocab(tri_counter, min_freq=1)\n",
        "\n",
        "print(f\"Trigram vocab size = {len(tri_vocab)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUZRjEnzwmpI",
        "outputId": "2e9dc0ac-bca2-42df-ad8f-f8133ea0510a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trigram vocab size = 4045111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(x):\n",
        "    return [tri_vocab.stoi[s] for s in tokenizer(x)]\n",
        "\n",
        "tri_vec=encode(first_sentence)\n",
        "tri_vec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVfSDzD2xNtW",
        "outputId": "564a08f2-eea8-406a-e65a-b8fa962e1e8b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[609,\n",
              " 601,\n",
              " 2,\n",
              " 2505,\n",
              " 65419,\n",
              " 160,\n",
              " 96,\n",
              " 3,\n",
              " 1211,\n",
              " 14,\n",
              " 32,\n",
              " 15,\n",
              " 32,\n",
              " 16,\n",
              " 963004,\n",
              " 4,\n",
              " 609,\n",
              " 534,\n",
              " 17,\n",
              " 10,\n",
              " 2015073,\n",
              " 7,\n",
              " 1037580,\n",
              " 4,\n",
              " 53,\n",
              " 7915,\n",
              " 1112,\n",
              " 475,\n",
              " 2]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def decode(x):\n",
        "    return [tri_vocab.itos[i] for i in x]\n",
        "\n",
        "decode(tri_vec)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b8eNqttxSuo",
        "outputId": "a75dcc63-d0e7-4cc2-ed0e-0b4474cdab38"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['wall',\n",
              " 'st',\n",
              " '.',\n",
              " 'bears',\n",
              " 'claw',\n",
              " 'back',\n",
              " 'into',\n",
              " 'the',\n",
              " 'black',\n",
              " '(',\n",
              " 'reuters',\n",
              " ')',\n",
              " 'reuters',\n",
              " '-',\n",
              " 'short-sellers',\n",
              " ',',\n",
              " 'wall',\n",
              " 'street',\n",
              " \"'\",\n",
              " 's',\n",
              " 'dwindling\\\\band',\n",
              " 'of',\n",
              " 'ultra-cynics',\n",
              " ',',\n",
              " 'are',\n",
              " 'seeing',\n",
              " 'green',\n",
              " 'again',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "cIeov6XWxgKY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}