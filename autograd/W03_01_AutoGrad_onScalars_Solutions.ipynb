{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating our own Auto Differentiation (AutoGrad) framework\n",
    "\n",
    "In this practical exercise we will build our own, very simple, Auto-differentiation (or AutoGrad) framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding the framework\n",
    "\n",
    "### Step 1: Define a class for our variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key idea is that we will define our own class of `Variable` which is basically the same as a scalar (a number). So our class is created by passing it a `value`, and it stores this value internally.\n",
    "\n",
    "But apart from being a placeholder for a number, we also want to keep track of the way every variable was created.\n",
    "\n",
    "For example, if a variable $c$ is the result of the addition of two variables $a$ and $b$: $c = a + b$, then we would say that $a$ and $b$ are \"parent\" variables of $c$, and $c$ is their \"child\". The way $c$ was created was by adding these two parent variables together.\n",
    "\n",
    "So apart from the value of the variable, we will also have to keep track of the parents, and on how each of them \"contributes\" to the creation of the variable - this is described by the local derivative associated with each of the parents, that tells us how a change in each of the parent variables translates into a change in the child variable.\n",
    "\n",
    "This is important in order to implement our backwards pass. Each parent defines a \"route\" through which the gradients coming into this variable will have to flow through. So we will define a list of `gradRoutes` that will contain the list of parent variables and their corresponding local derivatives. A variable created directly (not resulting by any operation over existing variables) will have an empty `gradRoutes`.\n",
    "\n",
    "Finally, we want each variable of ours to keep also the value of gradient of the quantity we are interested in with respect to that variable. We will create a placeholder for that as well, called `grad`. As seen before, this will accummulate the gradients that are backpropagated from the children of this variable when we implement the backpropagation algorithm. So we will initialise it to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Variable: #Simple variable. These are the leafs of our tree, they can request to have a gradient calculated, or not\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "        self.gradRoutes = [] # A variable by default has no grad Routes (was not created by anything, just defined)\n",
    "        self.grad = 0.0        \n",
    "        \n",
    "    def __str__(self):\n",
    "        return 'Value: {self.value}'.format(self=self)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from the `__init__()` function which stores the value passed to our class and initialises the `gradRoutes` and `grad` member variables, we have also overloaded the function that python uses to convert a class into a string representation: `__str__()`. This will allow us to print our class.\n",
    "\n",
    "We cannot do much yet with this class, apart from storing values into our variables and printing them out. Let's try this out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value: 4.3\n",
      "Value: 5.2\n"
     ]
    }
   ],
   "source": [
    "a = Variable(4.3)\n",
    "b = Variable(5.2)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define operations over our variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step would be to implement operations on our variables. Let's first define the operations for addition and multiplication.\n",
    "\n",
    "These will be functions that take two variables as input and produce a new (child) variable with a value equal to the sum or the product of the two inputs. Apart from the forward pass though, we should keep track of how this new variable was created: the two parent variables, and their corresponding local derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vAdd(A: Variable, B: Variable): # Addition\n",
    "    result = Variable(A.value + B.value) # Create a new Variable to store the result, and pass it the value = a + b\n",
    "          \n",
    "    #keep track of the parent variables, and of the local derivative associated with each one\n",
    "    result.gradRoutes.append((A, 1)) # dresult / dA = 1\n",
    "    result.gradRoutes.append((B, 1)) # dresult / dB = 1\n",
    "    \n",
    "    return result\n",
    "    \n",
    "def vMul(A: Variable, B: Variable): # Addition\n",
    "    result = Variable(A.value * B.value) # Create a new Variable to store the result, and pass it the value = a + b\n",
    "          \n",
    "    #keep track of the parent variables, and of the local derivative associated with each one\n",
    "    result.gradRoutes.append((A, B.value)) # dresult / dA = B\n",
    "    result.gradRoutes.append((B, A.value)) # dresult / dB = A\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for example to calculate $d = (a + b) * c$ we first need to calculate $(a + b)$ and then mutiply the result with $c$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value: 20\n"
     ]
    }
   ],
   "source": [
    "a = Variable(2) # a = 2\n",
    "b = Variable(3) # b = 3\n",
    "c = Variable(4) # c = 4\n",
    "\n",
    "#d = (a + b) * c = 20\n",
    "d = vMul(vAdd(a, b), c)\n",
    "\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Implement the backpropagation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to implement the backpropagation function. This starts with a child variable, and backpropagates gradients through the routes defined recursively. It uses the two rules that we saw before:\n",
    "\n",
    "- Accumulate the incoming gradients from the different grad routes that lead to a node (a variable). Each of the incoming gradients describe a different way in which the node affects the quantity of interest, so this sum will be the final gradient for the node\n",
    "- Multiply every incoming gradient with each of the local derivatives corresponding to parent variables (this would be the application of the chain rule), and continue the backpropagation through the corresponding route (for each of the parent variables)\n",
    "\n",
    "We update the `Variable` class accordingly. We also update the `__str___()` function to include also gradient information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Variable: #Simple variable. These are the leafs of our tree, they can request to have a gradient calculated, or not\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "        self.gradRoutes = [] # A variable by default has no grad Routes (was not created by anything, just defined)\n",
    "        self.grad = 0.0\n",
    "    \n",
    "    def backProp(self, route_val = 1.0):\n",
    "        # Add together the incoming gradients from the different routes that lead to a node - this will be the final gradient for the node\n",
    "        self.grad += route_val\n",
    "                \n",
    "        # For every parent variable and corresponding local derivative value that we have in the gradRoutes, continue with the gradient calculation\n",
    "        for variable, local_derivative_value in self.gradRoutes:\n",
    "            # Multiply the incoming gradient with the local derivatives corresponding the parent variable, and continue the backpropagation\n",
    "            variable.backProp(local_derivative_value * route_val)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'Value: {self.value}, Gradient: {self.grad}'.format(self=self)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>**Question:** Why did we set the default value of route_val equal to 1.0?</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*Answer: We set the default value of route_val = 1.0 as route is the final node of the tree. In the above example it is d. So, if we differentiate d w.r.t d its always 1. So it is set to 1.*\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should be all. If we want to calculate the derivative of the result with respect to any of the variables that participated in the calculation, we just need to call backprop on the result, and then read the derivatives out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result = 20\n",
      "The derivative of the result with respect to a is: 4.0\n",
      "The derivative of the result with respect to b is: 4.0\n",
      "The derivative of the result with respect to c is: 5.0\n"
     ]
    }
   ],
   "source": [
    "a = Variable(2)           # a = 2\n",
    "b = Variable(3)           # b = 3\n",
    "c = Variable(4)           # c = 4\n",
    "res = vMul(vAdd(a, b), c) # res = (a + b) * c = 20\n",
    "\n",
    "print(\"Result =\", res.value)\n",
    "\n",
    "# Call backprop on the result\n",
    "res.backProp()\n",
    "\n",
    "# Now all variables should contain in their \"grad\" the derivative d(res) / d(variable)\n",
    "print(\"The derivative of the result with respect to a is:\", a.grad)\n",
    "print(\"The derivative of the result with respect to b is:\", b.grad)\n",
    "print(\"The derivative of the result with respect to c is:\", c.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following example, variable $a$ affects the result through two different routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result = 28\n",
      "The derivative of the result with respect to a is: 11.0\n",
      "The derivative of the result with respect to b is: 4.0\n",
      "The derivative of the result with respect to c is: 4.0\n"
     ]
    }
   ],
   "source": [
    "a = Variable(4)  # a = 4\n",
    "b = Variable(3)  # b = 3\n",
    "c = vAdd(a, b)   # c = 4 + 3\n",
    "res = vMul(a, c) # res = a * c = 28\n",
    "\n",
    "print(\"Result =\", res.value)\n",
    "\n",
    "# Call backprop on the result\n",
    "res.backProp()\n",
    "\n",
    "# Now all variables should contain in their \"grad\" the derivative d(res) / d(variable)\n",
    "print(\"The derivative of the result with respect to a is:\", a.grad)\n",
    "print(\"The derivative of the result with respect to b is:\", b.grad)\n",
    "# Also for intermediate results\n",
    "print(\"The derivative of the result with respect to c is:\", c.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>**Question:** Can you now use this setup to calculate the derivative of $c$ with respect to $a$ and $b$?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The derivative of c with respect to a is: 1.0\n",
      "The derivative of c with respect to b is: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Your Code Here\n",
    "\n",
    "a = Variable(4)  # a = 4\n",
    "b = Variable(3)  # b = 3\n",
    "c = vAdd(a, b)   # c = 4 + 3\n",
    "res = vMul(a, c) # res = a * c = 28\n",
    "\n",
    "# Call backprop on the variable we are interested in: c. This will calculate the derivatives of c with respect to everything that it depends on\n",
    "c.backProp()\n",
    "\n",
    "print(\"The derivative of c with respect to a is:\", a.grad)\n",
    "print(\"The derivative of c with respect to b is:\", b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final touches\n",
    "\n",
    "If you understood how this works up to here, then you should be already good to go. But since we want to use our auto grad to do some practical work, we will continue working on it a bit, to make it a bit more usable and complete it with more operations. Many of the subsequent steps are quite \"engineering\" in nature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving usability: overloading operators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course this is still highly incomplete, very inefficient and not very usable. Lets first improve a usability issue. Instead of having to call different functions for the operations like `res = vMul(a, c)`, we would like to be able to directly write them down like `res = a * b`. To achieve this, we should overload [Python's special functions for operator overloading](https://docs.python.org/3/reference/datamodel.html#emulating-numeric-types).\n",
    "\n",
    "Here's how to do this for the addition and multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Variable: #Simple variable. These are the leafs of our tree, they can request to have a gradient calculated, or not\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "        self.gradRoutes = [] # A variable by default has no grad Routes (was not created by anything, just defined)\n",
    "        self.grad = 0.0\n",
    "    \n",
    "    def backProp(self, route_val = 1.0):\n",
    "        # Add together the incoming gradients from the different routes that lead to a node - this will be the final gradient for the node\n",
    "        self.grad += route_val\n",
    "                \n",
    "        # For every parent variable and corresponding local derivative value that we have in the gradRoutes, continue with the gradient calculation\n",
    "        for variable, local_derivative_value in self.gradRoutes:\n",
    "            # Multiply the incoming gradient with the local derivatives corresponding the parent variable, and continue the backpropagation\n",
    "            variable.backProp(local_derivative_value * route_val)\n",
    "            \n",
    "    def __add__(self, b):\n",
    "        return vAdd(self, b)\n",
    "        \n",
    "    def __mul__(self, b):\n",
    "        return vMul(self, b)            \n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'Value: {self.value}, Gradient: {self.grad}'.format(self=self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result = 28\n",
      "The derivative of the result with respect to a is: 11.0\n",
      "The derivative of the result with respect to b is: 4.0\n",
      "The derivative of the result with respect to c is: 4.0\n"
     ]
    }
   ],
   "source": [
    "a = Variable(4)  # a = 4\n",
    "b = Variable(3)  # b = 3\n",
    "c = a + b        # c = 4 + 3\n",
    "res = a * c      # res = a * c = 28\n",
    "\n",
    "print(\"Result =\", res.value)\n",
    "\n",
    "# Call backprop on the result\n",
    "res.backProp()\n",
    "\n",
    "# Now all variables should contain in their \"grad\" the derivative d(res) / d(variable)\n",
    "print(\"The derivative of the result with respect to a is:\", a.grad)\n",
    "print(\"The derivative of the result with respect to b is:\", b.grad)\n",
    "# Also for intermediate results\n",
    "print(\"The derivative of the result with respect to c is:\", b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zeroing gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A last thing to note is that once we call `backProp`, our gradients are calculated and our variables are now \"dirty\" in the sense that if we call backprop again, the new result will be added to the previous one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The derivative of the result with respect to a is: 11.0\n",
      "The derivative of the result with respect to b is: 4.0\n",
      "Second time\n",
      "The derivative of the result with respect to a is: 22.0\n",
      "The derivative of the result with respect to b is: 8.0\n"
     ]
    }
   ],
   "source": [
    "a = Variable(4)   # a = 4\n",
    "b = Variable(3)   # b = 3\n",
    "res = (a + b) * a # res = a * c = 28\n",
    "\n",
    "# Call backprop on the result\n",
    "res.backProp()\n",
    "print(\"The derivative of the result with respect to a is:\", a.grad)\n",
    "print(\"The derivative of the result with respect to b is:\", b.grad)\n",
    "\n",
    "# Call backprop on the result once more\n",
    "print(\"Second time\")\n",
    "res.backProp()\n",
    "print(\"The derivative of the result with respect to a is:\", a.grad)\n",
    "print(\"The derivative of the result with respect to b is:\", b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will actually turn out to be quite useful, e.g. when we want to accumulate weight gradients over different samples in our learning loops (see next), but we need a way to control it.\n",
    "\n",
    "To avoid this, we should reset the gradients to zero before we call `backProp` again. We can do it one by one for every variable, but we will also implement a function that does this recursively from the child node we backProped from all the way to the parents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Variable: #Simple variable. These are the leafs of our tree, they can request to have a gradient calculated, or not\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "        self.gradRoutes = [] # A variable by default has no grad Routes (was not created by anything, just defined)\n",
    "        self.grad = 0.0\n",
    "    \n",
    "    def backProp(self, route_val = 1.0):\n",
    "        # Add together the incoming gradients from the different routes that lead to a node - this will be the final gradient for the node\n",
    "        self.grad += route_val\n",
    "                \n",
    "        # For every parent variable and corresponding local derivative value that we have in the gradRoutes, continue with the gradient calculation\n",
    "        for variable, local_derivative_value in self.gradRoutes:\n",
    "            # Multiply the incoming gradient with the local derivatives corresponding the parent variable, and continue the backpropagation\n",
    "            variable.backProp(local_derivative_value * route_val)\n",
    "\n",
    "    def zeroGrad(self):\n",
    "        self.grad = 0.0\n",
    "        \n",
    "    def zeroGradsRecursively(self):\n",
    "        self.zeroGrad()\n",
    "        for variable, _ in self.gradRoutes:\n",
    "            variable.zeroGradsRecursively()\n",
    "            \n",
    "    def __add__(self, b):\n",
    "        return vAdd(self, b)\n",
    "        \n",
    "    def __mul__(self, b):\n",
    "        return vMul(self, b)           \n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'Value: {self.value}, Gradient: {self.grad}'.format(self=self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The derivative of the result with respect to a is: 11.0\n",
      "The derivative of the result with respect to b is: 4.0\n",
      "Second time\n",
      "The derivative of the result with respect to a is: 11.0\n",
      "The derivative of the result with respect to b is: 4.0\n"
     ]
    }
   ],
   "source": [
    "a = Variable(4)   # a = 4\n",
    "b = Variable(3)   # b = 3\n",
    "res = (a + b) * a # res = a * c = 28\n",
    "\n",
    "# Call backprop on the result\n",
    "res.backProp()\n",
    "print(\"The derivative of the result with respect to a is:\", a.grad)\n",
    "print(\"The derivative of the result with respect to b is:\", b.grad)\n",
    "\n",
    "# Zero gradients\n",
    "res.zeroGradsRecursively()\n",
    "\n",
    "# Call backprop on the result once more\n",
    "print(\"Second time\")\n",
    "res.backProp()\n",
    "print(\"The derivative of the result with respect to a is:\", a.grad)\n",
    "print(\"The derivative of the result with respect to b is:\", b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Improvements - Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are number of ways we can improve our simple network. The most important is probably being able to work with vectors and matrices - we will look into this in the next notebook. Before that, there are still a lot of things to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### <font color=blue>Excercise 1:</font>\n",
    "    \n",
    "<font color=blue>We usually do not require gradients for all our variables. If we could indicate which variables require gradients, then we could only keep track of the routes that lead to these variables only and drop all the rest. This would be a huge improvement in resources and speed (number of calculations). Can you add this functionality?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer - Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, add a new boolean member variable to indicate whether a variable requires its gradient calculated or not. We will initialise this by default to `False`, so we will need to indicate it when gradient calculation is needed.\n",
    "\n",
    "We will also update the `__str__()` method to inform about whether gradient is available or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Variable: #Simple variable. These are the leafs of our tree, they can request to have a gradient calculated, or not\n",
    "    def __init__(self, value, requiresGrad = False):\n",
    "        self.value = value\n",
    "        self.gradRoutes = [] # A variable by default has no grad Routes (was not created by anything, just defined)\n",
    "        self.grad = 0.0\n",
    "        self.requiresGrad = requiresGrad\n",
    "    \n",
    "    def backProp(self, route_val = 1.0):\n",
    "        # Add together the incoming gradients from the different routes that lead to a node - this will be the final gradient for the node\n",
    "        self.grad += route_val\n",
    "                \n",
    "        # For every parent variable and corresponding local derivative value that we have in the gradRoutes, continue with the gradient calculation\n",
    "        for variable, local_derivative_value in self.gradRoutes:\n",
    "            # Multiply the incoming gradient with the local derivatives corresponding the parent variable, and continue the backpropagation\n",
    "            variable.backProp(local_derivative_value * route_val)\n",
    "\n",
    "    def zeroGrad(self):\n",
    "        self.grad = 0.0\n",
    "        \n",
    "    def zeroGradsRecursively(self):\n",
    "        self.zeroGrad()\n",
    "        for variable, _ in self.gradRoutes:\n",
    "            variable.zeroGradsRecursively()\n",
    "            \n",
    "    def __add__(self, b):\n",
    "        return vAdd(self, b)\n",
    "        \n",
    "    def __mul__(self, b):\n",
    "        return vMul(self, b)           \n",
    "    \n",
    "    def __str__(self):\n",
    "        if self.requiresGrad:\n",
    "            return 'Value: {self.value}, Gradient: {self.grad}'.format(self=self)\n",
    "        else:\n",
    "            return 'Value: {self.value}, Gradient not required'.format(self=self)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to make changes in the operation functions, to only keep track of parent variables that require their gradient calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vAdd(A: Variable, B: Variable): # Addition\n",
    "    result = Variable(A.value + B.value) # Create a new Variable to store the result, and pass it the value = a + b\n",
    "          \n",
    "    #keep track of the parent variables, and of the local derivative associated with each one\n",
    "    if A.requiresGrad:\n",
    "        result.requiresGrad = True; # if any of the parent variables requires grad, the result should also require it\n",
    "        result.gradRoutes.append((A, 1)) # dresult / dA = 1\n",
    "    if B.requiresGrad:\n",
    "        result.requiresGrad = True; # if any of the parent variables requires grad, the result should also require it\n",
    "        result.gradRoutes.append((B, 1)) # dresult / dB = 1\n",
    "    \n",
    "    return result\n",
    "    \n",
    "def vMul(A: Variable, B: Variable): # Addition\n",
    "    result = Variable(A.value * B.value) # Create a new Variable to store the result, and pass it the value = a + b\n",
    "          \n",
    "    #keep track of the parent variables, and of the local derivative associated with each one\n",
    "    if A.requiresGrad:\n",
    "        result.requiresGrad = True; # if any of the parent variables requires grad, the result should also require it\n",
    "        result.gradRoutes.append((A, B.value)) # dresult / dA = B\n",
    "    if B.requiresGrad:\n",
    "        result.requiresGrad = True; # if any of the parent variables requires grad, the result should also require it\n",
    "        result.gradRoutes.append((B, A.value)) # dresult / dB = A\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value: 4, Gradient: 11.0\n",
      "Value: 3, Gradient not required\n",
      "The derivative of the result with respect to a is: 11.0\n",
      "The derivative of the result with respect to b is: 0.0\n"
     ]
    }
   ],
   "source": [
    "a = Variable(4, requiresGrad = True)   # a = 4\n",
    "b = Variable(3)   # b = 3\n",
    "res = (a + b) * a # res = 28\n",
    "\n",
    "# Call backprop on the result\n",
    "res.backProp()\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "print(\"The derivative of the result with respect to a is:\", a.grad)\n",
    "print(\"The derivative of the result with respect to b is:\", b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### <font color=blue>Excercise 2:</font>\n",
    "    \n",
    "<font color=blue>We obviously need to implement more functions - implement the following functions:\n",
    "- Subtraction\n",
    "- Raising to a power\n",
    "- Division\n",
    "- Unary negation\n",
    "- The (natural) exponential function exp(x)\n",
    "- ... any other function you might want</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer - Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the functions for power, exponential and log. The division, subtraction and unary negation do not need a separate function, as we can define them directly based on the operations we have (we will do this in the next step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pointwise raise to power\n",
    "def vPow(A: Variable, exponent: float):\n",
    "    result = Variable(np.float_power(A.value, exponent))\n",
    "    \n",
    "    if A.requiresGrad:\n",
    "        result.requiresGrad = True; # if any of our dependencies requires grad, we should also require it\n",
    "        result.grad = 0.0 # initialise to zero\n",
    "        result.gradRoutes.append((A, exponent*np.float_power(A.value, exponent-1)))\n",
    "        \n",
    "    return result\n",
    "    \n",
    "#Pointwise exp()\n",
    "def vExp(A: Variable):\n",
    "    result = Variable(np.exp(A.value))\n",
    "    \n",
    "    if A.requiresGrad:\n",
    "        result.requiresGrad = True; # if any of our dependencies requires grad, we should also require it\n",
    "        result.grad = 0.0 # initialise to zero\n",
    "        result.gradRoutes.append((A, np.exp(A.value)))\n",
    "\n",
    "    return result\n",
    "                    \n",
    "#Pointwise Log\n",
    "def vLog(A: Variable):\n",
    "    result = Variable(np.log(A.value))\n",
    "    \n",
    "    if A.requiresGrad:\n",
    "        result.requiresGrad = True; # if any of our dependencies requires grad, we should also require it\n",
    "        result.grad = 0.0 # initialise to zero\n",
    "        result.gradRoutes.append((A, 1/A.value))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overload operator functions or add member functions for the above operations. Also overload the operator functions for the division, substraction and unary negation, and define them based on the operations we already have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Variable: #Simple variable. These are the leafs of our tree, they can request to have a gradient calculated, or not\n",
    "    def __init__(self, value, requiresGrad = False):\n",
    "        self.value = value\n",
    "        self.gradRoutes = [] # A variable by default has no grad Routes (was not created by anything, just defined)\n",
    "        self.grad = 0.0\n",
    "        self.requiresGrad = requiresGrad\n",
    "    \n",
    "    def backProp(self, route_val = 1.0):\n",
    "        # Add together the incoming gradients from the different routes that lead to a node - this will be the final gradient for the node\n",
    "        self.grad += route_val\n",
    "                \n",
    "        # For every parent variable and corresponding local derivative value that we have in the gradRoutes, continue with the gradient calculation\n",
    "        for variable, local_derivative_value in self.gradRoutes:\n",
    "            # Multiply the incoming gradient with the local derivatives corresponding the parent variable, and continue the backpropagation\n",
    "            variable.backProp(local_derivative_value * route_val)\n",
    "\n",
    "    def zeroGrad(self):\n",
    "        self.grad = 0.0\n",
    "        \n",
    "    def zeroGradsRecursively(self):\n",
    "        self.zeroGrad()\n",
    "        for variable, _ in self.gradRoutes:\n",
    "            variable.zeroGradsRecursively()\n",
    "            \n",
    "    def __add__(self, b):\n",
    "        return vAdd(self, b)\n",
    "        \n",
    "    def __sub__(self, b):\n",
    "        return vAdd(self, vMul(b, Variable(-1.0)))\n",
    "\n",
    "    def __mul__(self, b):\n",
    "        return vMul(self, b)\n",
    "    \n",
    "    def __pow__(self, exponent):\n",
    "        return vPow(self, exponent)\n",
    "    \n",
    "    def __truediv__(self, b):\n",
    "        return vMul(self, b**(-1.0))\n",
    "    \n",
    "    def __neg__(self):\n",
    "        return self * Variable(-1)\n",
    "    \n",
    "    def __str__(self):\n",
    "        if self.requiresGrad:\n",
    "            return 'Value: {self.value}, Gradient: {self.grad}'.format(self=self)\n",
    "        else:\n",
    "            return 'Value: {self.value}, Gradient not required'.format(self=self)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: Value: 2, Gradient: 9.0\n",
      "b: Value: 3, Gradient: 12.0\n"
     ]
    }
   ],
   "source": [
    "a = Variable(2, requiresGrad = True)\n",
    "b = Variable(3, requiresGrad = True)\n",
    "#c = a * pow(b, 2) # Or alternatively: c = a * (b ** 2)\n",
    "\n",
    "c = a * (b ** 2)\n",
    "\n",
    "c.backProp()\n",
    "print(\"a:\", a)\n",
    "print(\"b:\", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: Value: 10, Gradient: 74.2065795512883\n",
      "b: Value: 2, Gradient: -371.0328977564415\n"
     ]
    }
   ],
   "source": [
    "a = Variable(10, requiresGrad = True)\n",
    "b = Variable(2, requiresGrad = True)\n",
    "c = a/b\n",
    "d = vExp(c)\n",
    "\n",
    "d.backProp()\n",
    "\n",
    "print(\"a:\", a)\n",
    "print(\"b:\", b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### <font color=blue>Excercise 3:</font>\n",
    "    \n",
    "<font color=blue>Our operations currently accept only instances of our Variable class as inputs. So, if you wanted to calculate `a = b * 2` where `b` is an instance of our variable class and `2` is just a numerical constant you would get an error as our framework does not know how to multiply a `Variable` with a number. You should instead write `a = b * Variable(2)` to achieve this.</font>\n",
    "\n",
    "<font color=blue>Improve further the usability of our framework by allowing our functions to mix numbers and Variables in the same operation (look also into the overloads of the [reflected operands in python](https://docs.python.org/3/reference/datamodel.html#object.__radd__))</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer - Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a helper function to check if something is a variable, and if not try to convert it to one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _ensureVariable(x):\n",
    "    if isinstance(x, Variable):\n",
    "        return x\n",
    "    elif isinstance(x, int) or isinstance(x, float):\n",
    "        return Variable(x)\n",
    "    else:\n",
    "        raise TypeError(\"We do not know how to convert variable of type {} to <class '__main__.Variable'>\".format(type(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now review our function definitions to include a call to this helper function for every input variable. This will now allow us to use integers or floats in these functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vAdd(A, B): # Addition\n",
    "    A = _ensureVariable(A)\n",
    "    B = _ensureVariable(B)\n",
    "    result = Variable(A.value + B.value) # Create a new Variable to store the result, and pass it the value = a + b\n",
    "          \n",
    "    #keep track of the parent variables, and of the local derivative associated with each one\n",
    "    if A.requiresGrad:\n",
    "        result.requiresGrad = True; # if any of the parent variables requires grad, the result should also require it\n",
    "        result.gradRoutes.append((A, 1)) # dresult / dA = 1\n",
    "    if B.requiresGrad:\n",
    "        result.requiresGrad = True; # if any of the parent variables requires grad, the result should also require it\n",
    "        result.gradRoutes.append((B, 1)) # dresult / dB = 1\n",
    "    \n",
    "    return result\n",
    "    \n",
    "def vMul(A, B): # Addition\n",
    "    A = _ensureVariable(A)\n",
    "    B = _ensureVariable(B)    \n",
    "    result = Variable(A.value * B.value) # Create a new Variable to store the result, and pass it the value = a + b\n",
    "          \n",
    "    #keep track of the parent variables, and of the local derivative associated with each one\n",
    "    if A.requiresGrad:\n",
    "        result.requiresGrad = True; # if any of the parent variables requires grad, the result should also require it\n",
    "        result.gradRoutes.append((A, B.value)) # dresult / dA = B\n",
    "    if B.requiresGrad:\n",
    "        result.requiresGrad = True; # if any of the parent variables requires grad, the result should also require it\n",
    "        result.gradRoutes.append((B, A.value)) # dresult / dB = A\n",
    "    \n",
    "    return result\n",
    "\n",
    "#Pointwise raise to power\n",
    "def vPow(A, exponent: float):\n",
    "    A = _ensureVariable(A)\n",
    "    result = Variable(np.float_power(A.value, exponent))\n",
    "    \n",
    "    if A.requiresGrad:\n",
    "        result.requiresGrad = True; # if any of our dependencies requires grad, we should also require it\n",
    "        result.grad = 0.0 # initialise to zero\n",
    "        result.gradRoutes.append((A, exponent*np.float_power(A.value, exponent-1)))\n",
    "        \n",
    "    return result\n",
    "    \n",
    "#Pointwise exp()\n",
    "def vExp(A):\n",
    "    A = _ensureVariable(A)\n",
    "    result = Variable(np.exp(A.value))\n",
    "    \n",
    "    if A.requiresGrad:\n",
    "        result.requiresGrad = True; # if any of our dependencies requires grad, we should also require it\n",
    "        result.grad = 0.0 # initialise to zero\n",
    "        result.gradRoutes.append((A, np.exp(A.value)))\n",
    "\n",
    "    return result\n",
    "                    \n",
    "#Pointwise Log\n",
    "def vLog(A):\n",
    "    A = _ensureVariable(A)\n",
    "    result = Variable(np.log(A.value))\n",
    "    \n",
    "    if A.requiresGrad:\n",
    "        result.requiresGrad = True; # if any of our dependencies requires grad, we should also require it\n",
    "        result.grad = 0.0 # initialise to zero\n",
    "        result.gradRoutes.append((A, 1/A.value))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value: 68, Gradient not required\n"
     ]
    }
   ],
   "source": [
    "a = Variable(2)\n",
    "b = a * 34\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But note that we are only able to use numbers on the right... so, we can write `b = a + 34`, but we cannot write `b = 34 + a`. The reason is that in this second case, Python looks for a function of the integer class that knows how to add a Variable to the integer, but integers do not know how to do that.\n",
    "\n",
    "In order to solve this problem, we should also implement the *reflected* versions of our operator functions, for `__add__()` we need to implement `__radd__()`, for `__mul__()` we need to implement `__rmul__()` etc.\n",
    "\n",
    "Then, when python does not find a way to add an integer and a Variable, it will ask the Variable class whether it knows how to do it. These *reflected* operators will allow Python to do so. Be carefule, as the order of the operands matters in some operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Variable: #Simple variable. These are the leafs of our tree, they can request to have a gradient calculated, or not\n",
    "    def __init__(self, value, requiresGrad = False):\n",
    "        self.value = value\n",
    "        self.gradRoutes = [] # A variable by default has no grad Routes (was not created by anything, just defined)\n",
    "        self.grad = 0.0\n",
    "        self.requiresGrad = requiresGrad\n",
    "    \n",
    "    def backProp(self, route_val = 1.0):\n",
    "        # Add together the incoming gradients from the different routes that lead to a node - this will be the final gradient for the node\n",
    "        self.grad += route_val\n",
    "                \n",
    "        # For every parent variable and corresponding local derivative value that we have in the gradRoutes, continue with the gradient calculation\n",
    "        for variable, local_derivative_value in self.gradRoutes:\n",
    "            # Multiply the incoming gradient with the local derivatives corresponding the parent variable, and continue the backpropagation\n",
    "            variable.backProp(local_derivative_value * route_val)\n",
    "\n",
    "    def zeroGrad(self):\n",
    "        self.grad = 0.0\n",
    "        \n",
    "    def zeroGradsRecursively(self):\n",
    "        self.zeroGrad()\n",
    "        for variable, _ in self.gradRoutes:\n",
    "            variable.zeroGradsRecursively()\n",
    "            \n",
    "    def __add__(self, b):\n",
    "        return vAdd(self, b)\n",
    "\n",
    "    def __radd__(self, b):\n",
    "        return vAdd(self, b)\n",
    "    \n",
    "    def __sub__(self, b):\n",
    "        return vAdd(self, -1 * b)\n",
    "\n",
    "    def __rsub__(self, b):\n",
    "        return vAdd(b, -1 * self)\n",
    "    \n",
    "    def __mul__(self, b):\n",
    "        return vMul(self, b)\n",
    "    \n",
    "    def __rmul__(self, b): # BE CAREFUL WITH THIS ONE. While we deal with scalars, order is not a problem, when we deal with matrices, this will\n",
    "        return vMul(b, self)\n",
    "    \n",
    "    def __pow__(self, exponent):\n",
    "        return vPow(self, exponent)\n",
    "    \n",
    "    def __truediv__(self, b):\n",
    "        return vMul(self, pow(b, -1.0))\n",
    "    \n",
    "    def __rtruediv__(self, b):\n",
    "        return vMul(b, pow(self, -1.0))\n",
    "    \n",
    "    def __neg__(self):\n",
    "        return self * Variable(-1) \n",
    " \n",
    "    def __str__(self):\n",
    "        if self.requiresGrad:\n",
    "            return 'Value: {self.value}, Gradient: {self.grad}'.format(self=self)\n",
    "        else:\n",
    "            return 'Value: {self.value}, Gradient not required'.format(self=self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value: 0.0625, Gradient not required\n",
      "Value: 64, Gradient not required\n",
      "Value: 1024.0, Gradient not required\n",
      "Value: -32, Gradient not required\n"
     ]
    }
   ],
   "source": [
    "a = Variable(32)\n",
    "\n",
    "b = 2 / a\n",
    "print(b)\n",
    "\n",
    "b = 2 * a\n",
    "print(b)\n",
    "\n",
    "b = a ** 2\n",
    "print(b)\n",
    "\n",
    "b = -a\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### <font color=blue>Excercise 4:</font>\n",
    "    \n",
    "<font color=blue>Instead of keeping track of the local derivative value and doing the multiplication between the local derivative value and the incoming gradient explicitly, it is better to keep track of the actual function used to calculate the local derivative and combine it with the incoming one (the `route_val`). The idea is to keep a note of the recipe for calculating the local derivative value instead of the value itself.</font>\n",
    "\n",
    "<font color=blue>In our case, since we deal with scalars, this function would basically boil down to doing this multiplication with the `route_val` (applying the chain rule), but when we vectorise our inputs and start dealing with tensors instead of scalars (and `route_val` becomes a matrix), this will become a bit more complicated (a dot product). So, keeping track of the function instead of the local derivative value will allow us to easily extend this framework to this scenario.</font>\n",
    "\n",
    "<font color=blue>In addition, keeping a note of the function instead of the value, allows us to abstract away stuff. This basically means that we can build the computation graph first, with placeholder variables independently of specific input values, and then reuse it for different inputs. This is how many deep learning frameworks work.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer - Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To achieve this, instead of calculating a storing a local derivative value when we create a new variable through an operation, we will create and store a reference to a function that we will call `gradfn()`. This is a function defined locally (so we will reuse this name) for each variable that we add in the `gradRoutes`. Then during backpropagation, instead of directly multiplying the `route_val` with the stored value, we will call that function on the `route_val`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vAdd(A, B): # Addition\n",
    "    A = _ensureVariable(A)\n",
    "    B = _ensureVariable(B)\n",
    "    result = Variable(A.value + B.value) # Create a new Variable to store the result, and pass it the value = a + b\n",
    "          \n",
    "    #keep track of the parent variables, and of the local derivative associated with each one\n",
    "    if A.requiresGrad:\n",
    "        result.requiresGrad = True; # if any of the parent variables requires grad, the result should also require it\n",
    "        def gradfn(route_val):\n",
    "            return route_val # == route_val * 1, as here dresult / dA = 1\n",
    "        result.gradRoutes.append((A, gradfn)) \n",
    "    if B.requiresGrad:\n",
    "        result.requiresGrad = True; # if any of the parent variables requires grad, the result should also require it\n",
    "        def gradfn(route_val):\n",
    "            return route_val # == route_val * 1, as here dresult / dA = 1\n",
    "        result.gradRoutes.append((B, gradfn))\n",
    "    \n",
    "    return result\n",
    "    \n",
    "def vMul(A, B): # Addition\n",
    "    A = _ensureVariable(A)\n",
    "    B = _ensureVariable(B)    \n",
    "    result = Variable(A.value * B.value) # Create a new Variable to store the result, and pass it the value = a + b\n",
    "          \n",
    "    #keep track of the parent variables, and of the local derivative associated with each one\n",
    "    if A.requiresGrad:\n",
    "        result.requiresGrad = True; # if any of the parent variables requires grad, the result should also require it\n",
    "        def gradfn(route_val):\n",
    "            return route_val * B.value # dresult / dA = B\n",
    "        result.gradRoutes.append((A, gradfn)) \n",
    "    if B.requiresGrad:\n",
    "        result.requiresGrad = True; # if any of the parent variables requires grad, the result should also require it\n",
    "        def gradfn(route_val):\n",
    "            return route_val * A.value # dresult / dB = A\n",
    "        result.gradRoutes.append((B, gradfn)) \n",
    "    \n",
    "    return result\n",
    "\n",
    "#Pointwise raise to power\n",
    "def vPow(A, exponent: float):\n",
    "    A = _ensureVariable(A)\n",
    "    result = Variable(np.float_power(A.value, exponent))\n",
    "    \n",
    "    if A.requiresGrad:\n",
    "        result.requiresGrad = True; # if any of our dependencies requires grad, we should also require it\n",
    "        result.grad = 0.0 # initialise to zero\n",
    "        def gradfn(route_val):\n",
    "            return route_val * exponent*np.float_power(A.value, exponent-1)\n",
    "        result.gradRoutes.append((A, gradfn))\n",
    "        \n",
    "    return result\n",
    "    \n",
    "#Pointwise exp()\n",
    "def vExp(A):\n",
    "    A = _ensureVariable(A)\n",
    "    result = Variable(np.exp(A.value))\n",
    "    \n",
    "    if A.requiresGrad:\n",
    "        result.requiresGrad = True; # if any of our dependencies requires grad, we should also require it\n",
    "        result.grad = 0.0 # initialise to zero\n",
    "        def gradfn(route_val):\n",
    "            return route_val * np.exp(A.value)\n",
    "        result.gradRoutes.append((A, gradfn))\n",
    "\n",
    "    return result\n",
    "                    \n",
    "#Pointwise Log\n",
    "def vLog(A):\n",
    "    A = _ensureVariable(A)\n",
    "    result = Variable(np.log(A.value))\n",
    "    \n",
    "    if A.requiresGrad:\n",
    "        result.requiresGrad = True; # if any of our dependencies requires grad, we should also require it\n",
    "        result.grad = 0.0 # initialise to zero\n",
    "        def gradfn(route_val):\n",
    "            return route_val/A.value\n",
    "        result.gradRoutes.append((A, gradfn))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Variable: #Simple variable. These are the leafs of our tree, they can request to have a gradient calculated, or not\n",
    "    def __init__(self, value, requiresGrad = False):\n",
    "        self.value = value\n",
    "        self.gradRoutes = [] # A variable by default has no grad Routes (was not created by anything, just defined)\n",
    "        self.grad = 0.0\n",
    "        self.requiresGrad = requiresGrad\n",
    "    \n",
    "    def backProp(self, route_val = 1.0):\n",
    "        # Add together the incoming gradients from the different routes that lead to a node - this will be the final gradient for the node\n",
    "        self.grad += route_val\n",
    "                \n",
    "        # For every parent variable and corresponding local derivative value that we have in the gradRoutes, continue with the gradient calculation\n",
    "        for variable, gradfn in self.gradRoutes:\n",
    "            # Call the gradfn() of each of the variables in our gradRouts passing the incoming gradient, and continue the backpropagation with the returned value\n",
    "            variable.backProp(gradfn(route_val))\n",
    "\n",
    "    def zeroGrad(self):\n",
    "        self.grad = 0.0\n",
    "        \n",
    "    def zeroGradsRecursively(self):\n",
    "        self.zeroGrad()\n",
    "        for variable, _ in self.gradRoutes:\n",
    "            variable.zeroGradsRecursively()\n",
    "            \n",
    "    def __add__(self, b):\n",
    "        return vAdd(self, b)\n",
    "\n",
    "    def __radd__(self, b):\n",
    "        return vAdd(self, b)\n",
    "    \n",
    "    def __sub__(self, b):\n",
    "        return vAdd(self, vMul(b, Variable(-1.0)))\n",
    "\n",
    "    def __rsub__(self, b):\n",
    "        return vAdd(b, -1 * self)\n",
    "    \n",
    "    def __mul__(self, b):\n",
    "        return vMul(self, b)\n",
    "    \n",
    "    def __rmul__(self, b): # BE CAREFUL WITH THIS ONE. While we deal with scalars, order is not a problem, when we deal with matrices, this will\n",
    "        return vMul(self, b)\n",
    "    \n",
    "    def __pow__(self, exponent):\n",
    "        return vPow(self, exponent)\n",
    "    \n",
    "    def __truediv__(self, b):\n",
    "        return vMul(self, pow(b, -1.0))\n",
    "    \n",
    "    def __rtruediv__(self, b):\n",
    "        return vMul(b, pow(self, -1.0))\n",
    "    \n",
    "    def __neg__(self):\n",
    "        return self * Variable(-1)\n",
    " \n",
    "    def __str__(self):\n",
    "        if self.requiresGrad:\n",
    "            return 'Value: {self.value}, Gradient: {self.grad}'.format(self=self)\n",
    "        else:\n",
    "            return 'Value: {self.value}, Gradient not required'.format(self=self)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check it all works as intended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value: 0.0625, Gradient: 1.0\n",
      "Value: 32, Gradient: -0.001953125\n",
      "Value: 64, Gradient: 1.0\n",
      "Value: 32, Gradient: 2.0\n",
      "Value: 1024.0, Gradient: 1.0\n",
      "Value: 32, Gradient: 64.0\n"
     ]
    }
   ],
   "source": [
    "a = Variable(32, requiresGrad = True)\n",
    "\n",
    "b = 2 / a\n",
    "b.backProp()\n",
    "print(b)\n",
    "print(a)\n",
    "\n",
    "b = 2 * a\n",
    "b.zeroGradsRecursively()\n",
    "b.backProp()\n",
    "print(b)\n",
    "print(a)\n",
    "\n",
    "b = a ** 2\n",
    "b.zeroGradsRecursively()\n",
    "b.backProp()\n",
    "print(b)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### <font color=blue>Excercise 5:</font>\n",
    "    \n",
    "<font color=blue>Write some code to manually check that your gradient calculation is correct, using the property of:</font>\n",
    "\n",
    "$$\n",
    "f'(x) = \\frac {f(x+\\epsilon) - f(x-\\epsilon)}{2 \\epsilon}\n",
    "$$\n",
    "\n",
    "<font color=blue>where $\\epsilon$ is a very small number to approximately calculate the gradient. Then use it to calculate the derivative of the function $f(x) = 21 * x^3$ at $x=3.2$. Double check that our framework gives you the same result.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer - Exercise 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myFunc(x):\n",
    "    return 21 * (x**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "645.1200000000001\n"
     ]
    }
   ],
   "source": [
    "# First, define x as a Variable and use our backProp to get the gradient\n",
    "x = Variable(3.2, requiresGrad = True)\n",
    "f = myFunc(x)\n",
    "\n",
    "f.backProp()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "645.1199396906304\n"
     ]
    }
   ],
   "source": [
    "# Now, define x as a float and manually calculate the gradient\n",
    "epsilon = 0.0000000001\n",
    "\n",
    "f1 = myFunc(3.2+epsilon)\n",
    "f2 = myFunc(3.2-epsilon)\n",
    "\n",
    "ExperimentalGrad = (f1 - f2)/(2 * epsilon)\n",
    "print(ExperimentalGrad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
